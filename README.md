# CS 182 Final Project: In-Context Learning of Moore Machines

**ğŸš§ GITHUB COPILOT SCAFFOLDING - TO BE EDITED AND CHANGED LATER ğŸš§**

This repository contains the scaffolding for our CS 182 final project on in-context learning of finite state machines using transformer models. This initial framework was generated with GitHub Copilot to provide a solid foundation for team collaboration.

## ğŸ¯ Project Overview

We study how transformer models can learn to simulate Moore machines through in-context learning, focusing on:

- **Constrained FSM Parameters**: 5 states, 5-8 actions, 4-8 transitions with self-loops
- **Small Transformer Models**: Optimized for 2-3 layer experiments
- **AdamW Optimizer**: Single optimizer focus to reduce experimental scope
- **Frozen Layer Experiments**: Test whether only the final linear layer can solve ICL

## ğŸ—ï¸ Repository Structure

```
CS182_FinalProject/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fsm/              # Moore machine implementation
â”‚   â”œâ”€â”€ training/         # PyTorch models & training loops
â”‚   â”œâ”€â”€ utils/           # Visualization & analysis tools
â”‚   â””â”€â”€ experiments/     # Experiment runners
â”œâ”€â”€ configs/             # YAML configuration files
â”œâ”€â”€ scripts/            # Training automation & testing
â”œâ”€â”€ notebooks/          # Jupyter notebooks for exploration
â””â”€â”€ requirements.txt    # Python dependencies
```

## ğŸš€ Quick Start

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Test the setup:**
   ```bash
   python scripts/test_setup.py
   ```

3. **Run experiments:**
   ```bash
   # 2-layer transformer (default)
   python experiments/run_experiment.py --config configs/base_config.yaml
   
   # 3-layer comparison
   python experiments/run_experiment.py --config configs/3layer_config.yaml
   
   # Frozen layer experiment (only final layer trains)
   python experiments/run_experiment.py --config configs/frozen_layers_config.yaml
   ```

## ğŸ§ª Experimental Configurations

### Base Configuration (`configs/base_config.yaml`)
- 2-layer transformer with 4 attention heads
- 128 model dimension, optimized for efficiency
- Standard training with all parameters trainable

### 3-Layer Configuration (`configs/3layer_config.yaml`) 
- 3-layer transformer for comparison
- Same hyperparameters for fair comparison

### Frozen Layer Configuration (`configs/frozen_layers_config.yaml`)
- **Tests core hypothesis**: Can only the final linear layer solve ICL?
- Freezes all transformer layers and embeddings
- Only the `lm_head` (final linear layer) remains trainable

## ğŸ”¬ Key Features

### Moore Machine Implementation
- Exactly 5 states (constraint from project scope)
- Variable 5-8 actions per machine
- 4-8 state transitions including self-loops
- Automatic validation of constraint compliance

### Transformer Architecture
- Decoder-only architecture with causal masking
- Multi-head attention with positional encoding
- Configurable freezing for ablation studies
- Parameter counting and frozen parameter tracking

### Training Framework
- AdamW optimizer with warmup and cosine annealing
- Gradient clipping and automatic checkpointing
- Optional Weights & Biases integration
- Comprehensive evaluation metrics

### Visualization Tools
- FSM diagram generation with NetworkX
- Training curve plotting
- Attention pattern visualization
- Performance analysis utilities

## ğŸ“Š Planned Experiments

1. **Baseline Performance**: 2-layer vs 3-layer transformers
2. **Frozen Layer Analysis**: Test if only final layer can solve ICL
3. **Scaling Studies**: Model size vs performance trade-offs
4. **Complexity Analysis**: FSM complexity vs learning difficulty
5. **Attention Visualization**: What patterns do transformers learn?

## ğŸ› ï¸ Development Notes

This scaffolding was generated to provide:
- âœ… Complete project structure
- âœ… Constrained FSM generation matching project requirements
- âœ… Small transformer models (2-3 layers) for efficient experimentation
- âœ… Frozen parameter experiments for mechanistic analysis
- âœ… AdamW-only optimization (reduced scope)
- âœ… Comprehensive configuration system
- âœ… Ready-to-run examples and tests

## ğŸ‘¥ Team Collaboration

**This is initial scaffolding generated by GitHub Copilot.** The framework is designed to be:
- **Modular**: Easy to modify individual components
- **Configurable**: YAML-based experiment configuration
- **Extensible**: Clean interfaces for adding new features
- **Well-documented**: Comprehensive docstrings and comments

Feel free to modify, extend, or completely rewrite any part of this codebase as needed for our research goals!

## ğŸ“ Next Steps

1. Review and validate the FSM implementation
2. Run baseline experiments to confirm functionality
3. Extend analysis tools based on experimental needs
4. Add team-specific modifications and improvements
5. Scale up experiments as needed

---

*Generated with GitHub Copilot as starting scaffold - ready for team development!*