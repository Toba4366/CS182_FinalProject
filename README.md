# CS 182 Final Project: In-Context Learning of Moore Machines

**ğŸš§ GITHUB COPILOT SCAFFOLDING - TO BE EDITED AND CHANGED LATER ğŸš§**

This repository contains the scaffolding for our CS 182 final project on in-context learning of finite state machines using transformer models. This initial framework was generated with GitHub Copilot to provide a solid foundation for team collaboration.

## ğŸ¯ Project Overview

We study how transformer models can learn to simulate Moore machines through in-context learning, focusing on:

- **Constrained FSM Parameters**: 5 states, 5-8 actions, 4-8 transitions with self-loops
- **Small Transformer Models**: Optimized for 2-3 layer experiments
- **AdamW Optimizer**: Single optimizer focus to reduce experimental scope
- **Frozen Layer Experiments**: Test whether only the final linear layer can solve ICL

## ğŸ—ï¸ Repository Structure

```
CS182_FinalProject/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ fsm/              # Moore machine implementation
â”‚   â”œâ”€â”€ training/         # PyTorch models & training loops
â”‚   â”œâ”€â”€ utils/           # Visualization & analysis tools
â”‚   â””â”€â”€ experiments/     # Experiment runners
â”œâ”€â”€ configs/             # YAML configuration files
â”œâ”€â”€ scripts/            # Training automation & testing
â”œâ”€â”€ notebooks/          # Jupyter notebooks for exploration
â””â”€â”€ requirements.txt    # Python dependencies
```

## ğŸš€ Quick Start

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Test the setup:**
   ```bash
   python scripts/test_imports.py    # Test basic imports
   python scripts/run_quick_training.py  # Run a quick training
   ```

3. **View training curves:**
   ```bash
   jupyter notebook notebooks/training_analysis.ipynb
   # Or run in VS Code with Jupyter extension
   ```

4. **Run experiments:**
   ```bash
   # 2-layer transformer (default)
   python scripts/run_quick_training.py
   
   # Or use the full experiment runner:
   python -m experiments.run_experiment --config configs/base_config.yaml
   
   # 3-layer comparison
   python -m experiments.run_experiment --config configs/3layer_config.yaml
   
   # Frozen layer experiment (only final layer trains)
   python -m experiments.run_experiment --config configs/frozen_layers_config.yaml
   ```

## ğŸ§ª Experimental Configurations

### Base Configuration (`configs/base_config.yaml`)
- 2-layer transformer with 4 attention heads
- 128 model dimension, optimized for efficiency
- Standard training with all parameters trainable

### 3-Layer Configuration (`configs/3layer_config.yaml`) 
- 3-layer transformer for comparison
- Same hyperparameters for fair comparison

### Frozen Layer Configuration (`configs/frozen_layers_config.yaml`)
- **Tests core hypothesis**: Can only the final linear layer solve ICL?
- Freezes all transformer layers and embeddings
- Only the `lm_head` (final linear layer) remains trainable

## ğŸ”¬ Key Features

### Moore Machine Implementation
- Exactly 5 states (constraint from project scope)
- Variable 5-8 actions per machine
- 4-8 state transitions including self-loops
- Automatic validation of constraint compliance

### Transformer Architecture
- Decoder-only architecture with causal masking
- Multi-head attention with positional encoding
- Configurable freezing for ablation studies
- Parameter counting and frozen parameter tracking

### Training Framework
- AdamW optimizer with warmup and cosine annealing
- Gradient clipping and automatic checkpointing
- Optional Weights & Biases integration
- Comprehensive evaluation metrics

### Visualization Tools
- FSM diagram generation with NetworkX
- Training curve plotting
- Attention pattern visualization
- Performance analysis utilities

## ğŸ“Š Planned Experiments

1. **Baseline Performance**: 2-layer vs 3-layer transformers
2. **Frozen Layer Analysis**: Test if only final layer can solve ICL
3. **Scaling Studies**: Model size vs performance trade-offs
4. **Complexity Analysis**: FSM complexity vs learning difficulty
5. **Attention Visualization**: What patterns do transformers learn?

## ğŸ› ï¸ Development Notes

This scaffolding was generated to provide:
- âœ… Complete project structure with working imports
- âœ… Constrained FSM generation matching project requirements
- âœ… Small transformer models (2-3 layers) for efficient experimentation
- âœ… Frozen parameter experiments for mechanistic analysis
- âœ… AdamW-only optimization (reduced scope)
- âœ… Comprehensive configuration system
- âœ… Ready-to-run examples and training visualization
- âœ… Jupyter notebook for training analysis

**Latest Update**: Fixed all import path issues, implemented proper loss computation for in-context learning, and validated training pipeline with working visualization.

## ğŸ‘¥ Team Collaboration

**This is initial scaffolding generated by GitHub Copilot.** The framework is designed to be:
- **Modular**: Easy to modify individual components
- **Configurable**: YAML-based experiment configuration
- **Extensible**: Clean interfaces for adding new features
- **Well-documented**: Comprehensive docstrings and comments

Feel free to modify, extend, or completely rewrite any part of this codebase as needed for our research goals!

## ğŸ“ Next Steps

1. âœ… **Setup Complete**: All imports working, training pipeline validated
2. **Run baseline experiments** to confirm full functionality
3. **Test frozen layer hypothesis** using `configs/frozen_layers_config.yaml`
4. **Analyze training curves** in the provided Jupyter notebook
5. **Extend analysis tools** based on experimental needs
6. **Add team-specific modifications** and improvements
7. **Scale up experiments** as computational resources allow

## ğŸ”§ **Troubleshooting**

- **Import errors**: Run `python scripts/test_imports.py` to validate setup
- **Training issues**: Try `python scripts/run_quick_training.py` for a minimal test
- **Visualization**: Use `notebooks/training_analysis.ipynb` for plotting
- **Configuration**: Check YAML configs in `configs/` directory

---

*Generated with GitHub Copilot as starting scaffold - ready for team development!*