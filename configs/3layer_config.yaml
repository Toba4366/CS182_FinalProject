# 3-Layer Transformer Configuration
# Tests performance with slightly larger but still efficient model

# Random seed for reproducibility
seed: 42

# Data configuration
data:
  train_machines: 500       # Standard dataset size
  val_machines: 100         # Validation machines
  examples_per_machine: 5   # Number of demonstration examples per machine
  sequence_length: 8        # Sequence length
  test_sequence_length: 12  # Test sequence length
  vocab_size: 64           # Vocabulary size

# Model configuration - 3 LAYERS
model:
  max_seq_len: 128         # Sequence length
  d_model: 128             # Model dimension
  num_heads: 4             # Number of attention heads
  num_layers: 3            # 3 layers for comparison with 2-layer
  d_ff: 256               # Feed-forward dimension
  dropout: 0.1            # Dropout rate
  freeze_layers: false     # Normal training
  freeze_embeddings: false # Normal training

# Training configuration
training:
  learning_rate: 1e-4      # Learning rate
  weight_decay: 0.01       # Weight decay
  warmup_steps: 500        # Warmup steps
  max_steps: 5000         # Training steps
  batch_size: 32          # Batch size
  eval_steps: 200         # Evaluation frequency
  save_steps: 500         # Save frequency
  gradient_clip_norm: 1.0  # Gradient clipping

# Logging configuration
use_wandb: false           # Whether to use Weights & Biases
wandb_project: "moore-icl-3layer" # W&B project name