# Frozen Layer Experiment Configuration
# Tests whether only the final linear layer can solve in-context learning

# Random seed for reproducibility
seed: 42

# Data configuration
data:
  train_machines: 200       # Smaller dataset for this specific test
  val_machines: 50          # Reduced for focused experiment
  examples_per_machine: 5   # Number of demonstration examples per machine
  sequence_length: 8        # Short sequences for focused test
  test_sequence_length: 12  # Test sequence length
  vocab_size: 64           # Reduced vocabulary size

# Model configuration - FROZEN LAYERS
model:
  max_seq_len: 128         # Reduced for faster experiments
  d_model: 128             # Model dimension
  num_heads: 4             # Number of attention heads
  num_layers: 2            # Minimal layers for the test
  d_ff: 256               # Feed-forward dimension
  dropout: 0.1            # Dropout rate
  freeze_layers: true      # FREEZE all transformer layers
  freeze_embeddings: true  # FREEZE embeddings too - only lm_head trains

# Training configuration
training:
  learning_rate: 5e-4      # Higher learning rate since only final layer trains
  weight_decay: 0.0        # No weight decay for this focused experiment
  warmup_steps: 100        # Minimal warmup
  max_steps: 2000         # Shorter training for this test
  batch_size: 64          # Larger batch since model is mostly frozen
  eval_steps: 100         # Frequent evaluation
  save_steps: 200         # Frequent saves
  gradient_clip_norm: 1.0  # Gradient clipping norm

# Logging configuration
use_wandb: false           # Whether to use Weights & Biases
wandb_project: "moore-icl-frozen" # W&B project name